{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"custom_dataset_dcase.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN0qe5emTJrCFgMmg9t0894"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NcK7L1OA1Tgu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1628355817750,"user_tz":-60,"elapsed":26311,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"6b02c39d-37b3-4572-8d3a-0402cc5b2ac5"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsefAZfHCDud"},"source":["!pip install torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1chFvuLA4EqQ","executionInfo":{"status":"ok","timestamp":1628356157275,"user_tz":-60,"elapsed":356,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"fae50ff2-f990-4ee0-a5c7-079dbad63739"},"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","import os\n","import torch\n","\n","class DCASE_Dataset(Dataset):\n","\n","  def __init__(self,\n","                 annotations_file,\n","                 audio_dir,\n","                 transformation,\n","                 target_sample_rate,\n","                 num_samples,\n","                 device):\n","    self.annotations = pd.read_csv(annotations_file)\n","    self.audio_dir = audio_dir\n","    self.device = device\n","    self.transformation = transformation.to(self.device)\n","    self.target_sample_rate = target_sample_rate\n","    self.num_samples = num_samples\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","\n","  def __getitem__(self, index):\n","    audio_sample_path = self._get_audio_sample_path(index)\n","    label = self._get_audio_sample_label(index)\n","    filename = self._get_audio_sample_filename(index)\n","    signal, sr = torchaudio.load(audio_sample_path) \n","    signal = signal.to(self.device)\n","    signal = self._resample_if_necessary(signal, sr)\n","    signal = self._mix_down_if_necessary(signal)\n","    signal = self._cut_if_necessary(signal)\n","    signal = self._right_pad_if_necessary(signal)\n","    signal = self.transformation(signal) \n","    return signal, label, filename\n","\n","  def _cut_if_necessary(self, signal):\n","      if signal.shape[1] > self.num_samples:\n","          signal = signal[:, :self.num_samples]\n","      return signal\n","\n","  def _right_pad_if_necessary(self, signal):\n","      length_signal = signal.shape[1]\n","      if length_signal < self.num_samples:\n","          num_missing_samples = self.num_samples - length_signal\n","          last_dim_padding = (0, num_missing_samples)\n","          signal = torch.nn.functional.pad(signal, last_dim_padding)\n","      return signal\n","\n","  def _resample_if_necessary(self, signal, sr):\n","    if sr != self.target_sample_rate:\n","        resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).cuda()\n","        signal = resampler(signal)\n","    return signal\n","\n","  def _mix_down_if_necessary(self, signal):\n","    if signal.shape[0] > 1: \n","        signal = torch.mean(signal, dim=0, keepdim=True)\n","    return signal\n","\n","  def _get_audio_sample_path(self, index):\n","    fold = f\"{self.annotations.iloc[index, 1]}\"\n","    path = os.path.join(self.audio_dir, fold, f\"{self.annotations.iloc[index, 0]}.wav\")\n","    print(path)\n","    return path\n","\n","  def _get_audio_sample_label(self, index):\n","    return self.annotations.iloc[index, 2]\n","\n","  def _get_audio_sample_filename(self, index):\n","    return f\"{self.annotations.iloc[index, 0]}.wav\"\n","\n","if __name__ == \"__main__\":\n","\n","  ANNOTATIONS_FILE = '/content/drive/My Drive/DCASE_Datasets/labels/combined_metadata.csv'\n","  AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","  SAMPLE_RATE = 22050\n","  DURATION = 10\n","  NUM_SAMPLES = 22050 * DURATION\n","\n","  if torch.cuda.is_available():\n","    device = \"cuda\"\n","  else:\n","    device = \"cpu\"\n","  print(f\"Using device {device}\")\n","\n","  mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","      sample_rate=SAMPLE_RATE,\n","      n_fft=1024,\n","      hop_length=512,\n","      n_mels=64\n","      )\n","\n","  dcase_data = DCASE_Dataset(ANNOTATIONS_FILE, \n","                             AUDIO_DIR, \n","                             mel_spectrogram, \n","                             SAMPLE_RATE,\n","                             NUM_SAMPLES,\n","                             device)\n","\n","  print(f\"There are {len(dcase_data)} samples in the dataset.\")\n","\n","  signal, label, filename = dcase_data[555]\n","\n","  print(signal.shape, label, filename)\n","\n","  print(signal)\n","\n","\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using device cuda\n","There are 35690 samples in the dataset.\n","/content/drive/My Drive/DCASE_Datasets/audio/ff1010bird/184463.wav\n","torch.Size([1, 64, 431]) 1 184463.wav\n","tensor([[[1.6176e+01, 1.0194e+01, 3.0178e+00,  ..., 1.4385e+01,\n","          4.8207e+00, 1.4695e+01],\n","         [2.6568e+00, 2.3177e+00, 2.3524e+00,  ..., 8.7684e+00,\n","          2.5412e+00, 1.0991e+01],\n","         [5.0469e-01, 9.2547e-01, 1.0446e+00,  ..., 9.0136e-01,\n","          1.6648e+00, 1.6646e+00],\n","         ...,\n","         [1.1813e-02, 1.2692e-02, 2.3908e-02,  ..., 1.4758e-02,\n","          1.8534e-02, 1.6635e-02],\n","         [8.7183e-03, 1.2198e-02, 1.2751e-02,  ..., 1.3182e-02,\n","          1.5341e-02, 1.3538e-02],\n","         [4.9677e-03, 1.0508e-02, 1.0361e-02,  ..., 1.1760e-02,\n","          9.1403e-03, 9.3836e-03]]], device='cuda:0')\n"],"name":"stdout"}]}]}