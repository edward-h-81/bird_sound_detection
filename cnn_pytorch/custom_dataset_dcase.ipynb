{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"custom_dataset_dcase.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPe+Ap+Q6e00g6yWo/UDd6h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcK7L1OA1Tgu","executionInfo":{"status":"ok","timestamp":1628172501656,"user_tz":-60,"elapsed":25093,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"0dec7975-ede1-4638-b0de-4363543f77d7"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsefAZfHCDud"},"source":["!pip install torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1chFvuLA4EqQ","executionInfo":{"status":"ok","timestamp":1628174594358,"user_tz":-60,"elapsed":747,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"d4f62f68-2a53-431f-d8f6-25eb23501587"},"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","import os\n","\n","class DCASE_Dataset(Dataset):\n","\n","  def __init__(self, annotations_file, audio_dir):\n","    self.annotations = pd.read_csv(annotations_file)\n","    self.audio_dir = audio_dir\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","\n","  def __getitem__(self, index):\n","    audio_sample_path = self._get_audio_sample_path(index)\n","    label = self._get_audio_sample_label(index)\n","    filename = self._get_audio_sample_filename(index)\n","    signal, sr = torchaudio.load(audio_sample_path) \n","    return signal, label, filename\n","\n","  def _get_audio_sample_path(self, index):\n","    fold = f\"{self.annotations.iloc[index, 1]}\"\n","    path = os.path.join(self.audio_dir, fold, f\"{self.annotations.iloc[index, 0]}.wav\")\n","    return path\n","\n","  def _get_audio_sample_label(self, index):\n","    return self.annotations.iloc[index, 2]\n","\n","  def _get_audio_sample_filename(self, index):\n","    return f\"{self.annotations.iloc[index, 0]}.wav\"\n","\n","if __name__ == \"__main__\":\n","\n","  ANNOTATIONS_FILE = '/content/drive/My Drive/DCASE_Datasets/labels/combined_metadata.csv'\n","  AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","\n","  dcase_data = DCASE_Dataset(ANNOTATIONS_FILE, AUDIO_DIR)\n","\n","  print(f\"There are {len(dcase_data)} samples in the dataset.\")\n","\n","  signal, label, filename = dcase_data[8898]\n","\n","  print(signal.shape, label, filename)\n","\n","  print(signal)\n","\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["There are 35690 samples in the dataset.\n","torch.Size([1, 440752]) 0 2606faa5-9430-4fbf-a3a5.wav\n","tensor([[ 0.0130,  0.0131,  0.0126,  ..., -0.0596, -0.0476, -0.0338]])\n"],"name":"stdout"}]}]}