{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dcasedataset_background_noise_test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMR/mhuziU/fedCtTSqVN4j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zkiCZMxBnxA6","executionInfo":{"status":"ok","timestamp":1631209899644,"user_tz":-60,"elapsed":19845,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"c26fd64b-2d0e-42ba-a909-1215321a6ffd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"sezUCK7Wn-0Q"},"source":["!pip install torchaudio librosa boto3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMrXp_AJoGVi"},"source":["#@title Prepare data and utility functions. {display-mode: \"form\"}\n","#@markdown\n","#@markdown You do not need to look into this cell.\n","#@markdown Just execute once and you are good to go.\n","#@markdown\n","#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), which is licensed under Creative Commos BY 4.0.\n","\n","#-------------------------------------------------------------------------------\n","# Preparation of data and helper functions.\n","#-------------------------------------------------------------------------------\n","import io\n","import os\n","import math\n","import tarfile\n","import multiprocessing\n","\n","import scipy\n","import librosa\n","import boto3\n","from botocore import UNSIGNED\n","from botocore.config import Config\n","import requests\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import time\n","from IPython.display import Audio, display\n","\n","[width, height] = matplotlib.rcParams['figure.figsize']\n","if width < 10:\n","  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n","\n","_SAMPLE_DIR = \"_sample_data\"\n","SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n","SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n","\n","SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n","\n","SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n","SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n","\n","SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n","SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n","\n","SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n","SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n","\n","SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n","SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n","\n","SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n","SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n","SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","\n","S3_BUCKET = \"pytorch-tutorial-assets\"\n","S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","\n","YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n","os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n","os.makedirs(_SAMPLE_DIR, exist_ok=True)\n","\n","def _fetch_data():\n","  uri = [\n","    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n","    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n","    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n","    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n","    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n","    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n","    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n","  ]\n","  for url, path in uri:\n","    with open(path, 'wb') as file_:\n","      file_.write(requests.get(url).content)\n","\n","_fetch_data()\n","\n","def _download_yesno():\n","  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n","    return\n","  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n","\n","YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n","YESNO_DOWNLOAD_PROCESS.start()\n","\n","def _get_sample(path, resample=None):\n","  effects = [\n","    [\"remix\", \"1\"]\n","  ]\n","  if resample:\n","    effects.extend([\n","      [\"lowpass\", f\"{resample // 2}\"],\n","      [\"rate\", f'{resample}'],\n","    ])\n","  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n","\n","def get_speech_sample(*, resample=None):\n","  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n","\n","def get_sample(*, resample=None):\n","  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n","\n","def get_rir_sample(*, resample=None, processed=False):\n","  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n","  if not processed:\n","    return rir_raw, sample_rate\n","  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n","  rir = rir / torch.norm(rir, p=2)\n","  rir = torch.flip(rir, [1])\n","  return rir, sample_rate\n","\n","def get_noise_sample(*, resample=None):\n","  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n","\n","def print_stats(waveform, sample_rate=None, src=None):\n","  if src:\n","    print(\"-\" * 10)\n","    print(\"Source:\", src)\n","    print(\"-\" * 10)\n","  if sample_rate:\n","    print(\"Sample Rate:\", sample_rate)\n","  print(\"Shape:\", tuple(waveform.shape))\n","  print(\"Dtype:\", waveform.dtype)\n","  print(f\" - Max:     {waveform.max().item():6.3f}\")\n","  print(f\" - Min:     {waveform.min().item():6.3f}\")\n","  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n","  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n","  print()\n","  print(waveform)\n","  print()\n","\n","def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","  figure, axes = plt.subplots(num_channels, 1)\n","  if num_channels == 1:\n","    axes = [axes]\n","  for c in range(num_channels):\n","    axes[c].plot(time_axis, waveform[c], linewidth=1)\n","    axes[c].grid(True)\n","    if num_channels > 1:\n","      axes[c].set_ylabel(f'Channel {c+1}')\n","    if xlim:\n","      axes[c].set_xlim(xlim)\n","    if ylim:\n","      axes[c].set_ylim(ylim)\n","  figure.suptitle(title)\n","  plt.show(block=False)\n","\n","def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","  figure, axes = plt.subplots(num_channels, 1)\n","  if num_channels == 1:\n","    axes = [axes]\n","  for c in range(num_channels):\n","    axes[c].specgram(waveform[c], Fs=sample_rate)\n","    if num_channels > 1:\n","      axes[c].set_ylabel(f'Channel {c+1}')\n","    if xlim:\n","      axes[c].set_xlim(xlim)\n","  figure.suptitle(title)\n","  plt.show(block=False)\n","\n","def play_audio(waveform, sample_rate):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  if num_channels == 1:\n","    display(Audio(waveform[0], rate=sample_rate))\n","  elif num_channels == 2:\n","    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n","  else:\n","    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n","\n","def inspect_file(path):\n","  print(\"-\" * 10)\n","  print(\"Source:\", path)\n","  print(\"-\" * 10)\n","  print(f\" - File size: {os.path.getsize(path)} bytes\")\n","  print(f\" - {torchaudio.info(path)}\")\n","\n","def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Spectrogram (db)')\n","  axs.set_ylabel(ylabel)\n","  axs.set_xlabel('frame')\n","  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n","  if xmax:\n","    axs.set_xlim((0, xmax))\n","  fig.colorbar(im, ax=axs)\n","  plt.show(block=False)\n","\n","def plot_mel_fbank(fbank, title=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Filter bank')\n","  axs.imshow(fbank, aspect='auto')\n","  axs.set_ylabel('frequency bin')\n","  axs.set_xlabel('mel bin')\n","  plt.show(block=False)\n","\n","def get_spectrogram(\n","    n_fft = 400,\n","    win_len = None,\n","    hop_len = None,\n","    power = 2.0,\n","):\n","  waveform, _ = get_speech_sample()\n","  spectrogram = T.Spectrogram(\n","      n_fft=n_fft,\n","      win_length=win_len,\n","      hop_length=hop_len,\n","      center=True,\n","      pad_mode=\"reflect\",\n","      power=power,\n","  )\n","  return spectrogram(waveform)\n","\n","def plot_pitch(waveform, sample_rate, pitch):\n","  figure, axis = plt.subplots(1, 1)\n","  axis.set_title(\"Pitch Feature\")\n","  axis.grid(True)\n","\n","  end_time = waveform.shape[1] / sample_rate\n","  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n","  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n","\n","  axis2 = axis.twinx()\n","  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n","  ln2 = axis2.plot(\n","      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n","\n","  axis2.legend(loc=0)\n","  plt.show(block=False)\n","\n","def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n","  figure, axis = plt.subplots(1, 1)\n","  axis.set_title(\"Kaldi Pitch Feature\")\n","  axis.grid(True)\n","\n","  end_time = waveform.shape[1] / sample_rate\n","  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n","  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n","\n","  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n","  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n","  axis.set_ylim((-1.3, 1.3))\n","\n","  axis2 = axis.twinx()\n","  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n","  ln2 = axis2.plot(\n","      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n","\n","  lns = ln1 + ln2\n","  labels = [l.get_label() for l in lns]\n","  axis.legend(lns, labels, loc=0)\n","  plt.show(block=False)\n","\n","DEFAULT_OFFSET = 201\n","SWEEP_MAX_SAMPLE_RATE = 48000\n","DEFAULT_LOWPASS_FILTER_WIDTH = 6\n","DEFAULT_ROLLOFF = 0.99\n","DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n","\n","def _get_log_freq(sample_rate, max_sweep_rate, offset):\n","  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n","\n","  offset is used to avoid negative infinity `log(offset + x)`.\n","\n","  \"\"\"\n","  half = sample_rate // 2\n","  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n","  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n","\n","def _get_inverse_log_freq(freq, sample_rate, offset):\n","  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n","  half = sample_rate // 2\n","  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n","\n","def _get_freq_ticks(sample_rate, offset, f_max):\n","  # Given the original sample rate used for generating the sweep,\n","  # find the x-axis value where the log-scale major frequency values fall in\n","  time, freq = [], []\n","  for exp in range(2, 5):\n","    for v in range(1, 10):\n","      f = v * 10 ** exp\n","      if f < sample_rate // 2:\n","        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n","        time.append(t)\n","        freq.append(f)\n","  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n","  time.append(t_max)\n","  freq.append(f_max)\n","  return time, freq\n","\n","def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n","  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n","  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n","\n","  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n","  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n","  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n","\n","  figure, axis = plt.subplots(1, 1)\n","  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n","  plt.xticks(time, freq_x)\n","  plt.yticks(freq_y, freq_y)\n","  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n","  axis.set_ylabel('Waveform Frequency (Hz)')\n","  axis.xaxis.grid(True, alpha=0.67)\n","  axis.yaxis.grid(True, alpha=0.67)\n","  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n","  plt.show(block=True)\n","\n","def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n","    max_sweep_rate = sample_rate\n","    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n","    delta = 2 * math.pi * freq / sample_rate\n","    cummulative = torch.cumsum(delta, dim=0)\n","    signal = torch.sin(cummulative).unsqueeze(dim=0)\n","    return signal\n","\n","def benchmark_resample(\n","    method,\n","    waveform,\n","    sample_rate,\n","    resample_rate,\n","    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n","    rolloff=DEFAULT_ROLLOFF,\n","    resampling_method=DEFAULT_RESAMPLING_METHOD,\n","    beta=None,\n","    librosa_type=None,\n","    iters=5\n","):\n","  if method == \"functional\":\n","    begin = time.time()\n","    for _ in range(iters):\n","      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n","                 rolloff=rolloff, resampling_method=resampling_method)\n","    elapsed = time.time() - begin\n","    return elapsed / iters\n","  elif method == \"transforms\":\n","    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n","                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n","    begin = time.time()\n","    for _ in range(iters):\n","      resampler(waveform)\n","    elapsed = time.time() - begin\n","    return elapsed / iters\n","  elif method == \"librosa\":\n","    waveform_np = waveform.squeeze().numpy()\n","    begin = time.time()\n","    for _ in range(iters):\n","      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n","    elapsed = time.time() - begin\n","    return elapsed / iters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxRcKU2UoiFa","executionInfo":{"status":"ok","timestamp":1631209921744,"user_tz":-60,"elapsed":182,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"462f8e22-997b-44f2-da4b-1e4b1deec3f2"},"source":["import torch\n","import torchaudio\n","import torchaudio.functional as F\n","import torchaudio.transforms as T\n","\n","print(torch.__version__)\n","print(torchaudio.__version__)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n","0.9.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wCFfb_Rrpsh9xSKZhe4WbfsXW_XdFHvq"},"id":"kl--U6p4mfIQ","executionInfo":{"status":"ok","timestamp":1631212294077,"user_tz":-60,"elapsed":11816,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"37d31230-7af6-41fa-8a26-33ac9479ca66"},"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","import os\n","import torch\n","\n","class DCASE_Dataset(Dataset):\n","\n","  def __init__(self,\n","                 annotations_file,\n","                 audio_dir,\n","                 transformation,\n","                 target_sample_rate,\n","                 num_samples,\n","                 device):\n","    self.annotations = pd.read_csv(annotations_file)\n","    self.audio_dir = audio_dir\n","    self.device = device\n","    self.transformation = transformation.to(self.device)\n","    self.target_sample_rate = target_sample_rate\n","    self.num_samples = num_samples\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","\n","  def __getitem__(self, index):\n","    audio_sample_path = self._get_audio_sample_path(index)\n","    label = self._get_audio_sample_label(index)\n","    signal, sr = torchaudio.load(audio_sample_path) \n","    signal = signal.to(self.device)\n","    signal = self._resample_if_necessary(signal, sr)\n","    signal = self._mix_down_if_necessary(signal)\n","    signal = self._cut_if_necessary(signal)\n","    signal = self._right_pad_if_necessary(signal)\n","    signal = self._add_background_noise(signal)\n","    signal = self.transformation(signal) \n","    return signal, label\n","\n","  def _cut_if_necessary(self, signal):\n","      if signal.shape[1] > self.num_samples:\n","          signal = signal[:, :self.num_samples]\n","      return signal\n","\n","  def _right_pad_if_necessary(self, signal):\n","      length_signal = signal.shape[1]\n","      if length_signal < self.num_samples:\n","          num_missing_samples = self.num_samples - length_signal\n","          last_dim_padding = (0, num_missing_samples)\n","          signal = torch.nn.functional.pad(signal, last_dim_padding)\n","      return signal\n","\n","  def _resample_if_necessary(self, signal, sr):\n","    if sr != self.target_sample_rate:\n","        resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).cuda()\n","        signal = resampler(signal)\n","    return signal\n","\n","  def _mix_down_if_necessary(self, signal):\n","    if signal.shape[0] > 1: \n","        signal = torch.mean(signal, dim=0, keepdim=True)\n","    return signal\n","\n","  def _add_background_noise(self, signal):\n","\n","    bird = signal\n","    noise, sr = torchaudio.load('/content/drive/My Drive/torchaudio/warblr_nobird.wav')\n","    noise = noise.to(self.device)\n","    noise = self._resample_if_necessary(noise, sr)\n","    noise = self._mix_down_if_necessary(noise)\n","    noise = self._cut_if_necessary(noise)\n","    noise = self._right_pad_if_necessary(noise)\n","\n","    plot_waveform(noise.cpu(), self.target_sample_rate, title=\"Background noise\")\n","    plot_specgram(noise.cpu(), self.target_sample_rate, title=\"Background noise\")\n","    play_audio(noise.cpu(), self.target_sample_rate)\n","\n","    bird_power = bird.norm(p=2)\n","    noise_power = noise.norm(p=2)\n","\n","    for snr_db in [20, 10, 3]:\n","      snr = math.exp(snr_db / 10)\n","      scale = snr * noise_power / bird_power\n","      noisy_bird = (scale * bird + noise) / 2\n","\n","      plot_waveform(noisy_bird.cpu(), self.target_sample_rate, title=f\"SNR: {snr_db} [dB]\")\n","      plot_specgram(noisy_bird.cpu(), self.target_sample_rate, title=f\"SNR: {snr_db} [dB]\")\n","      play_audio(noisy_bird.cpu(), self.target_sample_rate)\n","\n","    return signal\n","\n","  def _get_audio_sample_path(self, index):\n","    fold = f\"{self.annotations.iloc[index, 1]}\"\n","    path = os.path.join(self.audio_dir, fold, f\"{self.annotations.iloc[index, 0]}.wav\")\n","    print(path)\n","    return path\n","\n","  def _get_audio_sample_label(self, index):\n","    return self.annotations.iloc[index, 2]\n","\n","if __name__ == \"__main__\":\n","\n","  ANNOTATIONS_FILE = '/content/drive/My Drive/DCASE_Datasets/labels/mini_metadata.csv'\n","  AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","  SAMPLE_RATE = 22050\n","  DURATION = 10\n","  NUM_SAMPLES = 22050 * DURATION\n","\n","  if torch.cuda.is_available():\n","    device = \"cuda\"\n","  else:\n","    device = \"cpu\"\n","  print(f\"Using device {device}\")\n","\n","  mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","      sample_rate=SAMPLE_RATE,\n","      n_fft=1024,\n","      hop_length=512,\n","      n_mels=64\n","      )\n","\n","  dcase_data = DCASE_Dataset(ANNOTATIONS_FILE, \n","                             AUDIO_DIR, \n","                             mel_spectrogram, \n","                             SAMPLE_RATE,\n","                             NUM_SAMPLES,\n","                             device)\n","\n","  print(f\"There are {len(dcase_data)} samples in the dataset.\")\n","\n","  signal, label = dcase_data[5]\n","\n","  print(signal.shape, label)\n","\n","  print(signal)\n","\n","\n"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}