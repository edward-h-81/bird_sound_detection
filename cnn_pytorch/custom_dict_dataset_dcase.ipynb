{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"custom_dict_dataset_dcase.ipynb","provenance":[],"authorship_tag":"ABX9TyMIQkrFuBZAafr+dgGDMxBA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcK7L1OA1Tgu","executionInfo":{"status":"ok","timestamp":1628166075182,"user_tz":-60,"elapsed":28142,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"8b122f41-8607-4ba9-974f-171f84f72089"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gsefAZfHCDud"},"source":["!pip install torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1chFvuLA4EqQ","executionInfo":{"status":"ok","timestamp":1628172241438,"user_tz":-60,"elapsed":493,"user":{"displayName":"Edward Hulme","photoUrl":"","userId":"12267249823946796482"}},"outputId":"7ceec7f0-1cc5-42c7-9bda-325bee24fa4e"},"source":["from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","import os\n","import librosa\n","\n","class DCASE_Data(Dataset):\n","\n","  def __init__(self, annotations_dict, audio_dir):\n","    self.annotations = annotations_dict \n","    self.audio_dir = audio_dir\n","\n","  def __len__(self, set):\n","    if set == \"birdvox\":\n","      annotations = pd.read_csv(self.annotations[\"birdvox\"]) \n","    elif set == \"freefield\":\n","      annotations = pd.read_csv(self.annotations[\"freefield\"]) \n","    elif set == \"warblr\":\n","      annotations = pd.read_csv(self.annotations[\"warblr\"])\n","\n","    return len(annotations)\n","\n","  def __getitem__(self, set, index):\n","    audio_sample_path, annotations = self._get_audio_sample_path(set, index)\n","    label = self._get_audio_sample_label(annotations, index)\n","    filename = self._get_filename(annotations, index)\n","    signal, sr = torchaudio.load(audio_sample_path) \n","    return signal, label, filename\n","\n","  def _get_audio_sample_path(self, set, index):\n","    if set == \"birdvox\":\n","      annotations = pd.read_csv(self.annotations[\"birdvox\"]) \n","      fold = f\"{annotations.iloc[index, 1]}\"\n","      path = os.path.join(self.audio_dir, fold, f\"{annotations.iloc[index, 0]}.wav\")\n","    elif set == \"freefield\":\n","      annotations = pd.read_csv(self.annotations[\"freefield\"]) \n","      fold = f\"{annotations.iloc[index, 1]}\"\n","      path = os.path.join(self.audio_dir, fold, f\"{annotations.iloc[index, 0]}.wav\")\n","    elif set == \"warblr\":\n","      annotations = pd.read_csv(self.annotations[\"warblr\"]) \n","      fold = f\"{annotations.iloc[index, 1]}\"\n","      path = os.path.join(self.audio_dir, fold, f\"{annotations.iloc[index, 0]}.wav\")\n","    \n","    return path, annotations\n","\n","  def _get_audio_sample_label(self, annotations, index):\n","    return annotations.iloc[index, 2]\n","\n","  def _get_filename(self, annotations, index):\n","    return annotations.iloc[index, 0]\n","\n","if __name__ == \"__main__\":\n","\n","  ANNOTATIONS_DICT = {\n","      \"birdvox\": '/content/drive/My Drive/DCASE_Datasets/labels/BirdVox-DCASE20k.csv',\n","      \"freefield\": '/content/drive/My Drive/DCASE_Datasets/labels/ff1010bird.csv',\n","      \"warblr\": '/content/drive/My Drive/DCASE_Datasets/labels/warblrb10k.csv'\n","  }\n","\n","  AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","\n","  data = DCASE_Data(ANNOTATIONS_DICT, AUDIO_DIR)\n","\n","\n","  print(f\"There are {data.__len__('birdvox')} samples in the dataset.\")\n","  signal, label, filename = data.__getitem__('birdvox', 0)\n","  print(signal.shape, label, filename)\n","  print(signal)\n","  print()\n","\n","  print(f\"There are {data.__len__('freefield')} samples in the dataset.\")\n","  signal, label, filename = data.__getitem__('freefield', 0)\n","  print(signal.shape, label, filename)\n","  print(signal)\n","  print()\n","\n","  print(f\"There are {data.__len__('warblr')} samples in the dataset.\")\n","  signal, label, filename = data.__getitem__('warblr', 0)\n","  print(signal.shape, label, filename)\n","  print(signal)\n","  print()\n"],"execution_count":90,"outputs":[{"output_type":"stream","text":["There are 20000 samples in the dataset.\n","torch.Size([1, 441000]) 1 00053d90-e4b9-4045-a2f1-f39efc90cfa9\n","tensor([[ 0.0056,  0.0042, -0.0105,  ...,  0.0031,  0.0017, -0.0012]])\n","\n","There are 7690 samples in the dataset.\n","torch.Size([1, 441000]) 0 64486\n","tensor([[-0.0729, -0.0900, -0.0634,  ...,  0.0149,  0.0164,  0.0174]])\n","\n","There are 8000 samples in the dataset.\n","torch.Size([1, 444416]) 1 759808e5-f824-401e-9058\n","tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0226, 0.0189]])\n","\n"],"name":"stdout"}]}]}