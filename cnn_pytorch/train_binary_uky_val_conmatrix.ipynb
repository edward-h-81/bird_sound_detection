{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_binary_uky_val_conmatrix.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNcwH4NPS71SF53e83DR4gQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"NKnZHom2daU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631440005910,"user_tz":-60,"elapsed":759,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"e8fce52c-3dc3-4359-a6bb-e5ec915132de"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"0Ho7hW9RdinU","executionInfo":{"status":"ok","timestamp":1631440008737,"user_tz":-60,"elapsed":187,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}}},"source":["import sys\n","sys.path.insert(0,'/content/drive/My Drive/MSc_Project_Colab/BAD_PyTorch/')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HRLxTMNJecEr"},"source":["**CNN**"]},{"cell_type":"code","metadata":{"id":"dMl1PHrYdmWs"},"source":["!pip install torchaudio librosa boto3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rxo9KzPxeZsc"},"source":["from torch import nn\n","from torchsummary import summary\n","\n","\n","class CNNNetwork(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=1,\n","                out_channels=16,\n","                kernel_size=3,\n","                stride=1,\n","                padding='valid'\n","            ),\n","            nn.BatchNorm2d(16),\n","            nn.LeakyReLU(0.001),\n","            nn.MaxPool2d(kernel_size=3)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=16,\n","                out_channels=16,\n","                kernel_size=3,\n","                stride=1,\n","                padding='valid'\n","            ),\n","            nn.BatchNorm2d(16),\n","            nn.LeakyReLU(0.001),\n","            nn.MaxPool2d(kernel_size=3)\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=16,\n","                out_channels=16,\n","                kernel_size=3,\n","                stride=1,\n","                padding='valid'\n","            ),\n","            nn.BatchNorm2d(16),\n","            nn.LeakyReLU(0.001),\n","            nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=0)\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(\n","                in_channels=16,\n","                out_channels=16,\n","                kernel_size=3,\n","                stride=1,\n","                padding='valid'\n","            ),\n","            nn.BatchNorm2d(16),\n","            nn.LeakyReLU(0.001),\n","            nn.MaxPool2d(kernel_size=(1, 3), stride=(1, 3), padding=0)\n","        )\n","        self.flatten = nn.Flatten()\n","        self.dropout = nn.Dropout(p=0.5)\n","        self.linearA = nn.Linear(448, 256)\n","        self.batchnormA = nn.BatchNorm1d(256)\n","        self.leakyrelu = nn.LeakyReLU(0.001)\n","        self.linearB = nn.Linear(256, 32)\n","        self.batchnormB = nn.BatchNorm1d(32)\n","\n","        self.linear = nn.Linear(32, 1)\n","        # self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, input_data):\n","        x = self.conv1(input_data)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.flatten(x)\n","        x = self.dropout(x)\n","        x = self.linearA(x)\n","        x = self.batchnormA(x)\n","        x = self.leakyrelu(x)\n","        x = self.dropout(x)\n","        x = self.linearB(x)\n","        x = self.batchnormB(x)\n","        x = self.leakyrelu(x)\n","        x = self.dropout(x)\n","        logits = self.linear(x)\n","\n","        # predictions = self.sigmoid(logits)\n","        return logits\n","\n","\n","if __name__ == \"__main__\":\n","    cnn = CNNNetwork()\n","    summary(cnn.cuda(), (1, 80, 698))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqG_xzawO4KY","executionInfo":{"status":"ok","timestamp":1631440039663,"user_tz":-60,"elapsed":3006,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}}},"source":["#@title Prepare data and utility functions. {display-mode: \"form\"}\n","#@markdown\n","#@markdown You do not need to look into this cell.\n","#@markdown Just execute once and you are good to go.\n","#@markdown\n","#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https://iqtlabs.github.io/voices/), which is licensed under Creative Commos BY 4.0.\n","\n","#-------------------------------------------------------------------------------\n","# Preparation of data and helper functions.\n","#-------------------------------------------------------------------------------\n","import io\n","import os\n","import math\n","import tarfile\n","import multiprocessing\n","\n","import scipy\n","import librosa\n","import boto3\n","from botocore import UNSIGNED\n","from botocore.config import Config\n","import requests\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import time\n","from IPython.display import Audio, display\n","\n","[width, height] = matplotlib.rcParams['figure.figsize']\n","if width < 10:\n","  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n","\n","_SAMPLE_DIR = \"_sample_data\"\n","SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n","SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n","\n","SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n","\n","SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n","SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n","\n","SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n","SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n","\n","SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n","SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n","\n","SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n","SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n","\n","SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n","SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n","SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","\n","S3_BUCKET = \"pytorch-tutorial-assets\"\n","S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n","\n","YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n","os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n","os.makedirs(_SAMPLE_DIR, exist_ok=True)\n","\n","def _fetch_data():\n","  uri = [\n","    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n","    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n","    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n","    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n","    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n","    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n","    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n","  ]\n","  for url, path in uri:\n","    with open(path, 'wb') as file_:\n","      file_.write(requests.get(url).content)\n","\n","_fetch_data()\n","\n","def _download_yesno():\n","  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n","    return\n","  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n","\n","YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n","YESNO_DOWNLOAD_PROCESS.start()\n","\n","def _get_sample(path, resample=None):\n","  effects = [\n","    [\"remix\", \"1\"]\n","  ]\n","  if resample:\n","    effects.extend([\n","      [\"lowpass\", f\"{resample // 2}\"],\n","      [\"rate\", f'{resample}'],\n","    ])\n","  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n","\n","def get_speech_sample(*, resample=None):\n","  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n","\n","def get_sample(*, resample=None):\n","  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n","\n","def get_rir_sample(*, resample=None, processed=False):\n","  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n","  if not processed:\n","    return rir_raw, sample_rate\n","  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n","  rir = rir / torch.norm(rir, p=2)\n","  rir = torch.flip(rir, [1])\n","  return rir, sample_rate\n","\n","def get_noise_sample(*, resample=None):\n","  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n","\n","def print_stats(waveform, sample_rate=None, src=None):\n","  if src:\n","    print(\"-\" * 10)\n","    print(\"Source:\", src)\n","    print(\"-\" * 10)\n","  if sample_rate:\n","    print(\"Sample Rate:\", sample_rate)\n","  print(\"Shape:\", tuple(waveform.shape))\n","  print(\"Dtype:\", waveform.dtype)\n","  print(f\" - Max:     {waveform.max().item():6.3f}\")\n","  print(f\" - Min:     {waveform.min().item():6.3f}\")\n","  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n","  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n","  print()\n","  print(waveform)\n","  print()\n","\n","def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","  figure, axes = plt.subplots(num_channels, 1)\n","  if num_channels == 1:\n","    axes = [axes]\n","  for c in range(num_channels):\n","    axes[c].plot(time_axis, waveform[c], linewidth=1)\n","    axes[c].grid(True)\n","    if num_channels > 1:\n","      axes[c].set_ylabel(f'Channel {c+1}')\n","    if xlim:\n","      axes[c].set_xlim(xlim)\n","    if ylim:\n","      axes[c].set_ylim(ylim)\n","  figure.suptitle(title)\n","  plt.show(block=False)\n","\n","def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  time_axis = torch.arange(0, num_frames) / sample_rate\n","\n","  figure, axes = plt.subplots(num_channels, 1)\n","  if num_channels == 1:\n","    axes = [axes]\n","  for c in range(num_channels):\n","    axes[c].specgram(waveform[c], Fs=sample_rate)\n","    if num_channels > 1:\n","      axes[c].set_ylabel(f'Channel {c+1}')\n","    if xlim:\n","      axes[c].set_xlim(xlim)\n","  figure.suptitle(title)\n","  plt.show(block=False)\n","\n","def play_audio(waveform, sample_rate):\n","  waveform = waveform.numpy()\n","\n","  num_channels, num_frames = waveform.shape\n","  if num_channels == 1:\n","    display(Audio(waveform[0], rate=sample_rate))\n","  elif num_channels == 2:\n","    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n","  else:\n","    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n","\n","def inspect_file(path):\n","  print(\"-\" * 10)\n","  print(\"Source:\", path)\n","  print(\"-\" * 10)\n","  print(f\" - File size: {os.path.getsize(path)} bytes\")\n","  print(f\" - {torchaudio.info(path)}\")\n","\n","def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Spectrogram (db)')\n","  axs.set_ylabel(ylabel)\n","  axs.set_xlabel('frame')\n","  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n","  if xmax:\n","    axs.set_xlim((0, xmax))\n","  fig.colorbar(im, ax=axs)\n","  plt.show(block=False)\n","\n","def plot_mel_fbank(fbank, title=None):\n","  fig, axs = plt.subplots(1, 1)\n","  axs.set_title(title or 'Filter bank')\n","  axs.imshow(fbank, aspect='auto')\n","  axs.set_ylabel('frequency bin')\n","  axs.set_xlabel('mel bin')\n","  plt.show(block=False)\n","\n","def get_spectrogram(\n","    n_fft = 400,\n","    win_len = None,\n","    hop_len = None,\n","    power = 2.0,\n","):\n","  waveform, _ = get_speech_sample()\n","  spectrogram = T.Spectrogram(\n","      n_fft=n_fft,\n","      win_length=win_len,\n","      hop_length=hop_len,\n","      center=True,\n","      pad_mode=\"reflect\",\n","      power=power,\n","  )\n","  return spectrogram(waveform)\n","\n","def plot_pitch(waveform, sample_rate, pitch):\n","  figure, axis = plt.subplots(1, 1)\n","  axis.set_title(\"Pitch Feature\")\n","  axis.grid(True)\n","\n","  end_time = waveform.shape[1] / sample_rate\n","  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n","  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n","\n","  axis2 = axis.twinx()\n","  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n","  ln2 = axis2.plot(\n","      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n","\n","  axis2.legend(loc=0)\n","  plt.show(block=False)\n","\n","def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n","  figure, axis = plt.subplots(1, 1)\n","  axis.set_title(\"Kaldi Pitch Feature\")\n","  axis.grid(True)\n","\n","  end_time = waveform.shape[1] / sample_rate\n","  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n","  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n","\n","  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n","  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n","  axis.set_ylim((-1.3, 1.3))\n","\n","  axis2 = axis.twinx()\n","  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n","  ln2 = axis2.plot(\n","      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n","\n","  lns = ln1 + ln2\n","  labels = [l.get_label() for l in lns]\n","  axis.legend(lns, labels, loc=0)\n","  plt.show(block=False)\n","\n","DEFAULT_OFFSET = 201\n","SWEEP_MAX_SAMPLE_RATE = 48000\n","DEFAULT_LOWPASS_FILTER_WIDTH = 6\n","DEFAULT_ROLLOFF = 0.99\n","DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n","\n","def _get_log_freq(sample_rate, max_sweep_rate, offset):\n","  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n","\n","  offset is used to avoid negative infinity `log(offset + x)`.\n","\n","  \"\"\"\n","  half = sample_rate // 2\n","  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n","  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n","\n","def _get_inverse_log_freq(freq, sample_rate, offset):\n","  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n","  half = sample_rate // 2\n","  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n","\n","def _get_freq_ticks(sample_rate, offset, f_max):\n","  # Given the original sample rate used for generating the sweep,\n","  # find the x-axis value where the log-scale major frequency values fall in\n","  time, freq = [], []\n","  for exp in range(2, 5):\n","    for v in range(1, 10):\n","      f = v * 10 ** exp\n","      if f < sample_rate // 2:\n","        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n","        time.append(t)\n","        freq.append(f)\n","  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n","  time.append(t_max)\n","  freq.append(f_max)\n","  return time, freq\n","\n","def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n","  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n","  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n","\n","  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n","  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n","  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n","\n","  figure, axis = plt.subplots(1, 1)\n","  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n","  plt.xticks(time, freq_x)\n","  plt.yticks(freq_y, freq_y)\n","  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n","  axis.set_ylabel('Waveform Frequency (Hz)')\n","  axis.xaxis.grid(True, alpha=0.67)\n","  axis.yaxis.grid(True, alpha=0.67)\n","  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n","  plt.show(block=True)\n","\n","def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n","    max_sweep_rate = sample_rate\n","    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n","    delta = 2 * math.pi * freq / sample_rate\n","    cummulative = torch.cumsum(delta, dim=0)\n","    signal = torch.sin(cummulative).unsqueeze(dim=0)\n","    return signal\n","\n","def benchmark_resample(\n","    method,\n","    waveform,\n","    sample_rate,\n","    resample_rate,\n","    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n","    rolloff=DEFAULT_ROLLOFF,\n","    resampling_method=DEFAULT_RESAMPLING_METHOD,\n","    beta=None,\n","    librosa_type=None,\n","    iters=5\n","):\n","  if method == \"functional\":\n","    begin = time.time()\n","    for _ in range(iters):\n","      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n","                 rolloff=rolloff, resampling_method=resampling_method)\n","    elapsed = time.time() - begin\n","    return elapsed / iters\n","  elif method == \"transforms\":\n","    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n","                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n","    begin = time.time()\n","    for _ in range(iters):\n","      resampler(waveform)\n","    elapsed = time.time() - begin\n","    return elapsed / iters\n","  elif method == \"librosa\":\n","    waveform_np = waveform.squeeze().numpy()\n","    begin = time.time()\n","    for _ in range(iters):\n","      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n","    elapsed = time.time() - begin\n","    return elapsed / iters"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i66PNF3wdtgM"},"source":["Train the model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1uuQy2JojS1aFDH-ZAIGtqn7P3hxfPAvr"},"id":"p7fQH7h5d0pH","executionInfo":{"status":"error","timestamp":1631440065870,"user_tz":-60,"elapsed":14929,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"16fac71f-bf81-42e0-b146-214f92ddf8a5"},"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import torchaudio\n","\n","from random import seed\n","from random import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","\n","from dcasedataset_background_noise import DCASE_Dataset\n","\n","ANNOTATIONS_FILE_TRAIN = '/content/drive/My Drive/DCASE_Datasets/labels/bvff_25percent.csv'\n","ANNOTATIONS_FILE_VAL = '/content/drive/My Drive/DCASE_Datasets/labels/bvff_val.csv'\n","AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","SAMPLE_RATE = 22050\n","DURATION = 10\n","NUM_SAMPLES = 22050 * DURATION\n","\n","\n","BATCH_SIZE = 16\n","EPOCHS = 50\n","LEARNING_RATE = 0.001\n","\n","def create_data_loader(train_data, val_data, batch_size):\n","    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","    val_dataloader = DataLoader(val_data, batch_size=batch_size)\n","    return train_dataloader, val_dataloader\n","\n","\n","def train_single_epoch(model, data_loader, loss_fn, optimiser, device, sigmoid):\n","    size = len(data_loader.dataset)\n","    stages_per_epoch = len(data_loader)\n","    average_loss, correct = 0, 0\n","    print(\"Dataset size: {}\".format(size))\n","    print(\"Stages per epoch {}\".format(stages_per_epoch))\n","    \n","    for input, target in data_loader:\n","        input, target = input.to(device), target.to(device)\n","\n","        for index, signal in enumerate(input):\n","          # seed(1)\n","          value = random()\n","\n","          if value > 0:\n","            signal = t1masking(signal)\n","            signal = t2masking(signal)\n","            signal = fmasking(signal)\n","            input[index] = signal\n","\n","\n","        def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n","          fig, axs = plt.subplots(1, 1)\n","          axs.set_title(title or 'Spectrogram')\n","          axs.set_ylabel(ylabel)\n","          axs.set_xlabel('frame')\n","          spec = spec.cpu()\n","          spec = spec[0,:,:]\n","          im = axs.imshow(spec, origin='lower', aspect=aspect)\n","          if xmax:\n","            axs.set_xlim((0, xmax))\n","          fig.colorbar(im, ax=axs)\n","          plt.show(block=False)\n","\n","        for signal_mod in input:\n","          plot_spectrogram(signal_mod)\n","\n","        # calculate train loss\n","        prediction = model(input)\n","        target = target.unsqueeze_(1)\n","        target = target.type(torch.cuda.FloatTensor)\n","        loss = loss_fn(prediction, target)\n","        print(f\"Training loss: {loss.item()}\")\n","        average_loss += loss.item()\n","\n","        # calculate train accuracy\n","        pred = sigmoid(prediction)\n","        for i, p in enumerate(pred):\n","          if p > 0.5:\n","            p = 1\n","          else:\n","            p = 0\n","          if p == target[i]:\n","            correct += 1\n","\n","        # backpropagate error and update weights\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","\n","    print(f\"Average training loss: {average_loss / stages_per_epoch}\")\n","\n","    # train data for loss graph\n","    train_loss_y.append(average_loss / stages_per_epoch)\n","    array_len = len(train_x)\n","    train_x.append(array_len)\n","\n","    # train data for accuracy graph\n","    train_acc_y.append(correct / size)\n","\n","\n","def val_single_epoch(model, data_loader, loss_fn, device, sigmoid):\n","    size = len(data_loader.dataset)\n","    stages_per_epoch = len(data_loader)\n","    print(size)\n","    print(stages_per_epoch)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","      for input, target in data_loader:\n","        # calculate validation loss\n","        input, target = input.to(device), target.to(device)\n","        prediction = model(input)\n","        target = target.unsqueeze_(1)\n","        target = target.type(torch.cuda.FloatTensor)\n","        loss = loss_fn(prediction, target)\n","        test_loss += loss.item()\n","        print(f\"Validation loss: {loss.item()}\")\n","\n","        # calculate val accuracy\n","        pred = sigmoid(prediction)\n","        for i, p in enumerate(pred):\n","          if p > 0.5:\n","            p = 1\n","          else:\n","            p = 0\n","          if p == target[i]:\n","            correct += 1\n","\n","    print(f\"Average validation loss: {test_loss / stages_per_epoch}\")\n","\n","    # val data for loss graph\n","    val_loss_y.append(test_loss / stages_per_epoch)\n","    array_len = len(val_x)\n","    val_x.append(array_len)\n","\n","    # val data for accuracy graph\n","    val_acc_y.append(correct / size)\n","\n","def training_and_validation(model, train_loader, val_loader, loss_fn, optimiser, device, epochs, sigmoid):\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        train_single_epoch(model, train_loader, loss_fn, optimiser, device, sigmoid)\n","        val_single_epoch(model, val_loader, loss_fn, device, sigmoid)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","\n","\n","if __name__ == \"__main__\":\n","\n","    # containers for metrics\n","    train_loss_y = []\n","    train_x = []\n","    val_loss_y = []\n","    val_x = []\n","    train_acc_y = []\n","    val_acc_y = []\n","\n","    # sigmoid to calculate accuracy\n","    sigmoid = nn.Sigmoid()\n","\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","    print(f\"Using {device}\")\n","\n","    # instantiate dataset object and create data loader\n","    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","        sample_rate=SAMPLE_RATE,\n","        n_fft=1024,\n","        hop_length=316,\n","        n_mels=80,\n","        power=0.33\n","    )\n","\n","    t1masking = torchaudio.transforms.TimeMasking(time_mask_param=40)\n","    t2masking = torchaudio.transforms.TimeMasking(time_mask_param=40)\n","    fmasking = torchaudio.transforms.FrequencyMasking(freq_mask_param=10)\n","\n","    train_data = DCASE_Dataset(ANNOTATIONS_FILE_TRAIN,\n","                            AUDIO_DIR,\n","                            mel_spectrogram,\n","                            SAMPLE_RATE,\n","                            NUM_SAMPLES,\n","                            device)\n","    \n","    val_data = DCASE_Dataset(ANNOTATIONS_FILE_VAL,\n","                            AUDIO_DIR,\n","                            mel_spectrogram,\n","                            SAMPLE_RATE,\n","                            NUM_SAMPLES,\n","                            device)\n","    \n","    train_dataloader, val_dataloader = create_data_loader(train_data, val_data, BATCH_SIZE)\n","\n","    cnn = CNNNetwork().to(device)\n","    print(cnn)\n","\n","    # initialise loss funtion + optimiser\n","    # loss_fn = nn.BCELoss()\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    optimiser = torch.optim.Adam(cnn.parameters(), \n","                                 lr=LEARNING_RATE)\n","\n","    # train model\n","    training_and_validation(cnn, train_dataloader, val_dataloader, loss_fn, optimiser, device, EPOCHS, sigmoid)\n","\n","    # save model\n","    torch.save(cnn.state_dict(), \"/content/drive/My Drive/MSc_Project_Colab/BAD_PyTorch/cnn.pth\")\n","    print(\"Trained cnn saved at cnn.pth\")\n","\n","    # plot the loss over train and validation dataset\n","    plt.plot(train_x, train_loss_y)\n","    plt.plot(val_x, val_loss_y)\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend([\"Train\", \"Val\"], loc=2)\n","    plt.show()\n","\n","    # plot the accuracy over train and validation dataset\n","    plt.plot(train_x, train_acc_y)\n","    plt.plot(val_x, val_acc_y)\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.legend([\"Train\", \"Val\"], loc=2)\n","    plt.show()"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pNb1EJW4dwM7","executionInfo":{"status":"error","timestamp":1631440233493,"user_tz":-60,"elapsed":7776,"user":{"displayName":"Edward Hulme","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12267249823946796482"}},"outputId":"48b948a8-12dd-4649-b92e-4dd5c618932a"},"source":["import torch\n","import torchaudio\n","from torch import nn\n","\n","from random import seed\n","from random import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","import numpy as np\n","import pandas as pd\n","\n","from dcasedatasetcpu import DCASE_Dataset\n","# from cnnbinary_uky import CNNNetwork\n","# from train_binary import ANNOTATIONS_FILE, AUDIO_DIR, SAMPLE_RATE, DURATION, NUM_SAMPLES\n","\n","ANNOTATIONS_birdvox = '/content/drive/My Drive/DCASE_Datasets/labels/BirdVox-DCASE20k.csv'\n","ANNOTATIONS_warblr = '/content/drive/My Drive/DCASE_Datasets/labels/warblrb10k.csv'\n","ANNOTATIONS_freefield = '/content/drive/My Drive/DCASE_Datasets/labels/ff1010bird.csv'\n","ANNOTATIONS_mini = '/content/drive/My Drive/DCASE_Datasets/labels/mini_metadata.csv'\n","ANNOTATIONS_bvff_1200 = '/content/drive/My Drive/DCASE_Datasets/labels/bv_ff_1200.csv'\n","AUDIO_DIR = '/content/drive/My Drive/DCASE_Datasets/audio/'\n","SAMPLE_RATE = 22050\n","DURATION = 10\n","NUM_SAMPLES = 22050 * DURATION\n","THRESHOLD = 0.389085\n","\n","\n","class_mapping = [\n","    \"no-bird\",\n","    \"bird\"\n","]\n","\n","\n","def predict(model, input, target, class_mapping):\n","    model.eval()\n","    with torch.no_grad():\n","        predictions = model(input).cuda()\n","        sigmoid = nn.Sigmoid()\n","        predictions = sigmoid(predictions)\n","        print(predictions)\n","        pred_float = predictions[0]\n","        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n","        if predictions[0] > THRESHOLD:\n","\n","          # predicted_index = predictions[0].argmax(0)\n","          predicted_index = 1\n","        elif predictions[0] < THRESHOLD:\n","          predicted_index = 0\n","        print(predicted_index)\n","        predicted = class_mapping[predicted_index]\n","        expected = class_mapping[target]\n","    return predicted, expected, predicted_index, target, pred_float.item()\n","\n","\n","if __name__ == \"__main__\":\n","    # metrics\n","    preds = []\n","    targs = []\n","    pred_floats = []\n","\n","    # load back the model\n","    cnn = CNNNetwork()\n","    state_dict = torch.load(\"/content/drive/My Drive/MSc_Project_Colab/BAD_PyTorch/cnn_V8_bv50percent_ff_tfmask.pth\", map_location=torch.device('cpu'))\n","    cnn.load_state_dict(state_dict)\n","\n","    # load DCASE dataset\n","    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","        sample_rate=SAMPLE_RATE,\n","        n_fft=1024,\n","        hop_length=316,\n","        n_mels=80,\n","        power=0.33\n","    )\n","\n","    t1masking = torchaudio.transforms.TimeMasking(time_mask_param=40)\n","    t2masking = torchaudio.transforms.TimeMasking(time_mask_param=40)\n","    fmasking = torchaudio.transforms.FrequencyMasking(freq_mask_param=10)\n","\n","    dcase = DCASE_Dataset(ANNOTATIONS_warblr,\n","                            AUDIO_DIR,\n","                            mel_spectrogram,\n","                            SAMPLE_RATE,\n","                            NUM_SAMPLES,\n","                            \"cpu\")\n","\n","\n","    # get a sample from the dcase dataset for inference\n","    # num_files = len(dcase)\n","    num_files = 1400\n","    seed(1)\n","    count = 0\n","    index = 0\n","    correct = 0\n","    countnobird = 0\n","    countbird = 0\n","    while count < num_files:\n","      value = random()\n","      val_label = random()\n","\n","      input, target = dcase[index][0], dcase[index][1]\n","\n","\n","      if target == 1:\n","        if val_label < 0.67:\n","          index += 1\n","          continue\n","\n","      # if value > 0.5:\n","      #   input = t1masking(input)\n","      #   input = t2masking(input)\n","      #   input = fmasking(input)\n","      #   input = input\n","\n","      # def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n","      #     fig, axs = plt.subplots(1, 1)\n","      #     axs.set_title(title or 'Spectrogram')\n","      #     axs.set_ylabel(ylabel)\n","      #     axs.set_xlabel('frame')\n","      #     spec = spec.cpu()\n","      #     spec = spec[0,:,:]\n","      #     im = axs.imshow(spec, origin='lower', aspect=aspect)\n","      #     if xmax:\n","      #       axs.set_xlim((0, xmax))\n","      #     fig.colorbar(im, ax=axs)\n","      #     plt.show(block=False)\n","\n","      \n","      # plot_spectrogram(input)\n","\n","\n","      input.unsqueeze_(0)\n","\n","      index += 1\n","      count += 1\n","\n","      if target == 1:\n","        countbird += 1\n","      else:\n","        countnobird += 1\n","\n","    # make an inference\n","      predicted, expected, pred, targ, pred_float = predict(cnn, input, target,\n","                                  class_mapping)\n","      \n","      print(pred_float)\n","      \n","      preds.append(pred)\n","      targs.append(targ)\n","      pred_floats.append(pred_float)\n","\n","      if predicted == expected:\n","        correct += 1\n","      print(f\"Predicted: '{predicted}', expected: '{expected}'\")\n","      print()\n","\n","\n","    accuracy = correct / num_files\n","    print(accuracy)\n","    print(\"bird: {}\".format(countbird))\n","    print(\"no-bird: {}\".format(countnobird))\n","\n","\n","    # roc\n","    fpr, tpr, thresholds = roc_curve(targs, pred_floats)\n","    # print(\"fpr: {}, tpr: {}\".format(fpr, tpr, pos_label=2))\n","    plt.figure(1)\n","    plt.plot([0, 1], [0, 1], 'y--')\n","    plt.plot(fpr, tpr, marker='.')\n","    plt.xlabel('False positive rate')\n","    plt.ylabel('True positive rate')\n","    plt.title('ROC curve')\n","    plt.show\n","\n","    i = np.arange(len(tpr)) \n","    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'thresholds' : pd.Series(thresholds, index=i)})\n","    ideal_roc_thresh = roc.iloc[(roc.tf-0).abs().argsort()[:1]]  #Locate the point where the value is close to 0\n","    print(\"Ideal threshold is: \", ideal_roc_thresh['thresholds']) \n","\n","    # AUC\n","    auc_value = auc(fpr, tpr)\n","    print(\"Area under curve, AUC = \", auc_value)\n","\n","    # confusion matrix\n","    cm = confusion_matrix(targs, preds)\n","    print(cm)\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"no-bird\", \"bird\"])\n","    plt.figure(2)\n","    disp.plot(values_format='') \n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/759808e5-f824-401e-9058.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/759808e5-f824-401e-9058.wav\n","tensor([[0.6143]], device='cuda:0')\n","1\n","0.6143028140068054\n","Predicted: 'bird', expected: 'bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/1d94fc4a-1c63-4da0-9cac.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/1d94fc4a-1c63-4da0-9cac.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/bb0099ce-3073-4613-8557.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/bb0099ce-3073-4613-8557.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/c4c67e81-9aa8-4af4-8eb7.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/c4c67e81-9aa8-4af4-8eb7.wav\n","tensor([[0.7510]], device='cuda:0')\n","1\n","0.7509929537773132\n","Predicted: 'bird', expected: 'bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/ab322d4b-da69-4b06-a065.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/ab322d4b-da69-4b06-a065.wav\n","tensor([[0.3760]], device='cuda:0')\n","0\n","0.37596869468688965\n","Predicted: 'no-bird', expected: 'no-bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/519cfbe6-f804-4add-baa3.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/519cfbe6-f804-4add-baa3.wav\n","tensor([[0.7181]], device='cuda:0')\n","1\n","0.7181000113487244\n","Predicted: 'bird', expected: 'no-bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/6332d960-6f57-4ecc-8d1a.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/6332d960-6f57-4ecc-8d1a.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/db89b696-5ca0-4ca8-982a.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/db89b696-5ca0-4ca8-982a.wav\n","tensor([[0.5092]], device='cuda:0')\n","1\n","0.5092249512672424\n","Predicted: 'bird', expected: 'bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/a02ac7bc-5a29-40a1-89e1.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/a02ac7bc-5a29-40a1-89e1.wav\n","tensor([[0.9628]], device='cuda:0')\n","1\n","0.9628344178199768\n","Predicted: 'bird', expected: 'bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/6ce66c37-3a83-43b1-b0dd.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/6ce66c37-3a83-43b1-b0dd.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/126160c6-cd85-41f7-a5e7.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/126160c6-cd85-41f7-a5e7.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/19d149c7-98a8-48d2-921d.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/19d149c7-98a8-48d2-921d.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/4dd5046d-c962-4f02-a820.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/4dd5046d-c962-4f02-a820.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/479b90e3-85bf-403c-8298.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/479b90e3-85bf-403c-8298.wav\n","tensor([[0.7852]], device='cuda:0')\n","1\n","0.7851831912994385\n","Predicted: 'bird', expected: 'no-bird'\n","\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/3661273c-19b9-4ea0-abc5.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/3661273c-19b9-4ea0-abc5.wav\n","/content/drive/My Drive/DCASE_Datasets/audio/warblrb10k/5e8976e1-c7bf-45b7-b22b.wav\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-4a29043cf8a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mval_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m       \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/MSc_Project_Colab/BAD_PyTorch/dcasedatasetcpu.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0maudio_sample_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_sample_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_audio_sample_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_sample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msignal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resample_if_necessary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/backend/sox_io_backend.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     return torch.ops.torchaudio.sox_io_load_audio_file(\n\u001b[0;32m--> 153\u001b[0;31m         filepath, frame_offset, num_frames, normalize, channels_first, format)\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}